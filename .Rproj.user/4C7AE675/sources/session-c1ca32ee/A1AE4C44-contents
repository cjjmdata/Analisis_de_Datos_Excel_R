---
title: "Regresion lineal"
author: "CJJM"
format: 
  revealjs:
    multiplex: true
    logo: img/logo.png
    theme: ["pp.scss"]
    chalkboard: true
    slide-number: c/t
    incremental: true
    title-slide-attributes:
      data-background-image: img/portada.png
      data-background-size: cover  
editor: visual
---

```{r}
library(dplyr)
#library(UsingR)
library(ggplot2)

load("datos/pisa18_puntajes.RData")

```

# El Modelo Lineal

## Relación entre variables

Qué relaciones podrían existir entre las siguientes variables:

-   El salario de un trabajador y su escolaridad formal, años de experiencia laboral, semanas de capacitación y el género.
-   El rendimiento de un cultivo y la cantidad de fertilizante utilizado.
-   La calidad del vino y su acidez volátil, el contenido de alcohol, su pH, nivel de sulfatos y su tipo (tinto/blanco).
-   La altura de un hijo y la altura de sus padres.
-   El puntaje del examen de lectura y el puntaje del examen de matemáticas en la prueba PISA 2018.

::: notes
Q = - acidez volátil + contenido de alcohol - pH + nivel sulfatos + tinto.
:::

## Relación entre variables

Un diagrama de dispersión permite visualizar el tipo de relación que existe entre dos variables.

```{r, fig.align='center'}
set.seed(123)
x <- runif(200, 0, 10)
y1 <- 2*x + rnorm(200, 5, 2)
y2 <- -2*x + rnorm(200, 25, 2)
y3 <- x^2 + rnorm(200, 5, 2)
y4 <- rnorm(200, 5, 2)

# Crear un marco de 2x2 para las gráficas
par(mfrow = c(2, 2))

# Gráfica 1: Relación lineal positiva
plot(x, y1, main = "Relación Lineal Positiva", xlab = "x", ylab = "y")
# Gráfica 2: Relación lineal negativa
plot(x, y2, main = "Relación Lineal Negativa", xlab = "x", ylab = "y")
# Gráfica 3: Relación no lineal
plot(x, y3, main = "Relación No Lineal", xlab = "x", ylab = "y")
# Gráfica 4: Sin relación evidente
plot(x, y4, main = "Sin Relación Evidente", xlab = "x", ylab = "y")

set.seed(NULL)

```



## Relación entre variables: PISA 2018

El diagrama de dispersión entre los puntajes de los exámenes de matemáticas y lectura de la prueba PISA 2018 para México, muestra una relación estrecha y positiva.

¿Cómo podríamos resumir esta relación?

```{r}
plot(x = pisa18$PVMATH, y = pisa18$PVREAD, col = "#4ABF9C")
```

## La línea de regresión

Una línea puede resumir la relación, pero...

[¿Qué características debe tener esta línea?]{style="color: #316BAC;"}

```{r}

x <- pisa18$PVMATH
y <- pisa18$PVREAD
plot(x = pisa18$PVMATH, y = pisa18$PVREAD, col = "#4ABF9C")
abline(lm(y ~ x), col = "#316BAC", lwd = 2)

```

## La línea de regresión

Modelo determinista:

$$Y = \alpha + \beta x$$

Es la recta que pasa más cerca de todos los puntos, la recta de mejor ajuste: minimiza la suma de las desviaciones cuadráticas de los valores observados respecto de la recta.

```{r, fig.align='center'}

set.seed(123)
x <- rnorm(100, mean = 2, sd = 4)
y <- 0.5*x + rnorm(100, mean = 5, sd = 1)

# Ajustar el modelo de regresión lineal simple
modelo <- lm(y ~ x)

library(ggplot2)

ggplot(data = data.frame(x, y), aes(x = x, y = y)) +
  geom_point(col = "#FFFFFF") +
  labs(x = "X", y = "Y") +
  geom_hline(yintercept = 0, color = "GRAY", linewidth = 1) +
  geom_vline(xintercept = 0, color = "GRAY", linewidth = 1) +
  geom_segment(x = 2.3, y = 6, xend = 4, yend = 6, color = 2) +
  geom_segment(x = 4, y = 6, xend = 4, yend = 6.9, color = 2) +
  geom_smooth(method = "lm", se = FALSE) +
  theme(panel.background = element_rect(fill = "transparent"),
        axis.ticks = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank()) +
  annotate('text', label= expression(alpha), 
           x=-0.4, y=4.9, hjust=0.5, vjust=0, size=8) +
  annotate('text', label= expression(beta), 
           x=4.4, y=5.8, hjust=0.5, vjust=0, size=8) +
  coord_cartesian(xlim = c(-max(abs(x)), max(abs(x))),
                  ylim = c(0, max(abs(y))))

set.seed(NULL)

```


## La línea de regresión

Modelo probabilístico simple:

$$Y_i = \alpha + \beta_1x_i + u$$
Donde:
$Y_i = \alpha + \beta_1x_i$ es la recta de medias y $u$ es el error (la distancia entre cada punto y la recta de medias)


```{r, fig.align='center'}

set.seed(345)
datos <- select(pisa18, PVMATH, PVREAD)
muestra <- datos[sample(nrow(datos), 100, replace = FALSE),]

y <- muestra$PVREAD
x <- muestra$PVMATH

# Estimar el modelos de regresión
modelo <- lm(y ~ x)
# Obtener los residuos del modelo
residuos <- residuals(modelo)

# Integrar los datos por posición
datos <- data.frame(x, y, residuos)

# Crear la gráfica con ggplot2
ggplot(datos, aes(x = x, y = y)) +
  geom_point(color = "#063365") +
  geom_smooth(method = lm, se = FALSE, color = "#316BAC") +
  geom_hline(yintercept = mean(y), color = "#4ABF9C", size = 1) +
  geom_vline(xintercept = mean(x), color = "#4ABF9C", size = 1) +
  geom_linerange(aes(ymin = y, ymax = y - residuos), color = "#FF7E39") +
  labs(title = "Ajuste del Modelo de Regresión Lineal",
       x = "Variable independiente: puntaje en Matemáticas (X)", y = "Variable dependiente: puntaje en lectura (Y)") +
  annotate("text", x = mean(x) + 0.08 * min(x), y = max(y) - 0.1 * min(y), label = "Media de x", vjust = -1) +
  annotate("text", x = min(x) + 0.1 * min(x), y = mean(y) + 0.05 * min(y), label = "Media de y", hjust = 1)

set.seed(NULL)

```



## Supuestos sobre el error aleatorio $u$

::: {.nonincremental}
1. Son independientes uno de otro.
:::

```{r, fig.align='center'}

#| layout: [[45,-10, 45], [100]]

set.seed(91011)

n1 <- 30
t1 <- 1:n1
e_ind <- rnorm(n1, mean = 0, sd = 0.5)

n <- 20
t2 <- seq(0.2, n, 0.2)
e_dep1 <- sin(t2/10 * 3 * pi)

t3 <- 1:n
comp_aleatorio <- rnorm(n, mean = 0, sd = 0.1)
e_dep2 <- rep(c(-0.5, 0.5), n/2) + comp_aleatorio

par(mfrow = c(2, 2))

plot(t1, e_ind, xlab = "t", ylab = "Residual", main = "Independencia")
abline(h = 0, lty = "dashed", col = "red")

plot(t2, e_dep1, xlab = "t", ylab = "Residual", main = "Dependencia")
abline(h = 0, lty = "dashed", col = "red")

plot(t3, e_dep2, xlab = "t", ylab = "Residual", main = "Dependencia")
abline(h = 0, lty = "dashed", col = "red")


```



## Supuestos sobre el error aleatorio $u$

::: {.nonincremental}
2. $\bar{u} = 0$
3. Se distribuye de manera normal.
:::

```{r, fig.align='center'}

set.seed(123)

n <- 100
e_n <- rnorm(n, mean = 0, sd = 0.5)

par(mfrow = c(2, 2))
plot(1:n, e_n, xlab = "n", ylab = "Residual", main = "Errores")
plot(density(e_n), main = "Densidad de probabilidad de los errores")
qqnorm(e_n, main = "Gráfico Q-Q de los errores")
qqline(e_n)

set.seed(NULL)

```


## Supuestos sobre el error aleatorio $u$

::: {.nonincremental}
4. Tienen una varianza común = $\sigma^2$ (homocedasticidad).
:::


```{r, fig.align='center'}

set.seed(123)

n <- 100
e_n <- rnorm(n, mean = 0, sd = 0.5)

x <- 1:n
# Generar valores simulados de la variable dependiente con errores heterocedásticos
y <- (x/4) + rnorm(100, mean = 0, sd = x/8)
# Ajustar modelo de regresión lineal
modelo <- lm(y ~ x)
# Obtener los residuos del modelo
residuos <- residuals(modelo)

par(mfrow = c(1, 2))
plot(1:n, e_n, xlab = expression(hat(y)),
     ylab = "Residuos", main = "Homocedasticidad",
     axes = FALSE)
axis(1, labels = FALSE)
axis(2, labels = FALSE)

plot(fitted(modelo), residuos, xlab = expression(hat(y)),
     ylab = "Residuos", main = "Heterocedasticidad",
     axes = FALSE)
axis(1, labels = FALSE)
axis(2, labels = FALSE)

set.seed(NULL)

```


## Distribución condicional de la respuesta {visibility="hidden"}

![](img/grafica_01.png){.r-stretch}

Fuente: Ugarte, M. D., Militino, A. F., & Arnholt, A. T. (2008). Probability and statistics with R. CRC Press.



# Estimación de la recta


## Estimación de la recta

### Mínimos cuadrados

<br>
Objetivo: disminuir la suma de las desviaciones al cuadrado de los valores observados ($y_i$) y los valores pronosticados ($\hat{y}_i$).

<br>

$$SSE = \sum(y_i-\hat{y}_i)^2 = \sum(y_i - a - bx_i)^2$$

<br>

Donde SSE: suma de cuadrados del error


## Estimación de la recta

Para estimar la ecuación de la recta de mejor ajuste, utilizamos las siguientes fórmulas:


$$b = \frac{S_{xy}}{S_{xx}}; \quad \text{y,} \quad a = \bar{y} - b\bar{x}$$
Donde:

$$S_{xy} = \sum(x_i - \bar{x})(y_i-\bar{y}) = \sum x_iy_i-\frac{(\sum x_i)(\sum y_i)}{n}$$
$$S_{xx} = \sum(x_i-\bar{x})^2 = \sum x_i^2 - \frac{(\sum x_i)^2}{n}$$


# Análisis de la Varianza

## Variación total

El modelo de regresión plantea que la variable $y$ se relaciona linealmente con $x$, por tanto, la variabilidad total de $y$ se define como:

$$SS_{total} = \sum(y_i-\bar{y})^2 = \sum y_i^2 - \frac{(\sum y_i)^2}{n}$$

La variabilidad total se divide en dos componentes:

::: {.nonincremental}
- **SSR (suma de cuadrados para regresión):** mide la cantidad de variación explicada por el modelo (por la recta de regresión).
- **SSE (suma de cuadrados del error):** variación que no explica la variable independiente x (variación residual).
:::


## Variación total: componentes


$$SS_{total} = SSR + SSE$$


```{r, fig.align='center'}

set.seed(456)
datos <- select(pisa18, PVMATH, PVREAD)
muestra <- datos[sample(nrow(datos), 20, replace = FALSE),]

y <- muestra$PVREAD
x <- muestra$PVMATH

# Estimar el modelos de regresión
modelo <- lm(y ~ x)
# Obtener los residuos del modelo
residuos <- residuals(modelo)

# Integrar los datos por posición
datos <- data.frame(x, y, residuos)

# Crear la gráfica con ggplot2
ggplot(datos, aes(x = x, y = y)) +
  geom_point(color = "#063365") +
  geom_smooth(method = lm, se = FALSE, color = "#316BAC") +
  geom_hline(yintercept = mean(y), color = "#4ABF9C", size = 1) +
  geom_vline(xintercept = mean(x), color = "#4ABF9C", size = 1) +
  labs(title = "Ajuste del Modelo de Regresión Lineal",
       x = "Variable independiente: puntaje en Matemáticas (X)", y = "Variable dependiente: puntaje en lectura (Y)") +
  annotate("text", x = mean(x) + 0.08 * min(x), y = max(y) - 0.1 * min(y), label = "Media de x", vjust = -1) +
  annotate("text", x = min(x) + 0.1 * min(x), y = mean(y) + 0.05 * min(y), label = "Media de y", hjust = 1)

set.seed(NULL)

```



## Análisis de la Varianza

$$SSR = \sum(\hat{y}_i-\bar{y})^2 = \sum(a + bx_i - \bar{x})^2 = \sum(\bar{y}-b\bar{x}+bx_i-\bar{y})$$

$$= b^2\sum(x_i - \bar{x})^2 = \left( \frac{(S_{xy})}{S_{xx}}\right) ^2S_{xx}=\frac{(S_{xy})^2}{S_{xx}}$$

$$SSE = SS_{total}-SSR$$
Y el error medio cuadrático esta definido como:

$$MSE = s^2 = \frac{SSE}{n-2}$$

# Estimación


## R: función lm

Para estimar la regresión lineal en R utilizamos la función:

```
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
```

Los párametros mínimos que utilizaremos son:

`formula`: es la descripción del modelo que será ajustado, por ejemplo, `y ~ x1 + x2`.

`data`: es el dataframe donde se encuentran las variables.


## Ejemplo: estimación


Ajustar el modelo $$y = a + b x$$

Donde:  
  y : puntaje en el examen de lectura  
  x : puntaje en el examen de matemáticas
  

```{r}
#| echo: true

y <- pisa18$PVREAD
x <- pisa18$PVMATH

modelo1 <- lm(y ~ x)

modelo1
```



## Ejemplo: validación

Para poder validar el modelo, debemos recuperar todos los resultados:

:::: {.columns}

::: {.column width="55%"}

```{r}
#| echo: true

summary(modelo1)

```


:::

::: {.column width="45%"}

$\hat{READ} = 12.948698 + 0.997854 MAT$

::: {.nonincremental}
- Por cada punto que se incremente el resultado de matemáticas, se espera que el resultado de lectura se incremente 0.9979 puntos.
- Si el resultado de matemáticas = 0, se espera que lectura = 12.9487.
:::


:::

::::

Para rechazar $H_0: a = 0$, pvalue < al nivel de significancia seleccionado.  
Para rechazar $H_0: b = 0$, pvalue < al nivel de significancia seleccionado.


## Ejemplo: visualización

Para visualizar el modelos, podemos generar una gráfica con la librería ggplot2:

```{r}
#| echo: true
y <- pisa18$PVREAD
x <- pisa18$PVMATH

ggplot(pisa18, aes(x = x, y = y)) +
  geom_point(color = "#4ABF9C") +
  geom_smooth(method = 'lm', color = "#316BAC")

```


## Ejemplo: valoración del ajuste

¿Qué tan bien se ajusta la línea de regresión a los datos?

¿Cómo medir la intensidad de la relación entre la variable de respuesta ($y$) y la variable explicativa ($x$)?...

$$\frac{SSR}{SS_{total}} = \frac{(S_{xy})^2}{S_{xx}S_{yy}} = \left( \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}\right)^2 = R^2$$

## Ejemplo: coeficiente de determinación

**Coeficiente de determinación $R^2$:** .


## Ejemplo: análisis de residuales

El análisis gráfico de los residuales se obtienen con la siguiente función:

```{r}
#| echo: true

par(mfrow=c(2, 2))
plot(modelo1, las=1, col='#316BAC', which=1:3)

```


## Ejemplo: pronóstico

Uno de los principales usos del modelo de regresión lineal es el pronóstico.

Supongamos que queremos pronosticar el puntaje del examen de lectura de un estudiante que obtuvo 325 puntos en matemática, es decir:

$$E(READ|MAT = 325)$$


Podemos utilizar el resultado del modelo estimado: 

$\hat{READ} = 12.948698 + 0.997854 (325) = 337.251248$

O la función de R:

```{r}
#| echo: true

nuevo <- data.frame(x =c(325))
predict(object=modelo1, newdata=nuevo)

```



## Ejemplo: intervalos de confianza

Para estimar el valor promedio de $y$ cuando $x = x_0$: $E(y|x_0)$.


```{r}
#| echo: true
nuevo <- data.frame(x =c(325))
predict(object=modelo1, newdata=nuevo, interval="confidence", level=0.95)

```


Para predecir un valor particular de $y$ cuando $x = x_0$: nuevas observaciones.

```{r}
#| echo: true
nuevo <- data.frame(x =c(325))
predict(object=modelo1, newdata=nuevo, interval="prediction", level=0.95)

```


## Ejemplo: intervalos de confianza {visibility="hidden"}


```{r}
#| echo: true
future_y <- predict(object=modelo1, interval="prediction", level=0.95)
nuevos_datos <- cbind(pisa18, future_y)

ggplot(nuevos_datos, aes(x=pisa18$PVMATH, y=pisa18$PVREAD))+
    geom_point(color = "white") +
    geom_line(aes(y=lwr), color="red", linetype="dashed") +
    geom_line(aes(y=upr), color="red", linetype="dashed") +
    geom_smooth(method=lm, formula=y~x, se=TRUE, level=0.95, col='blue', fill='pink2') +
    theme_light()

```




# Modelo de regresión múltiple


## Modelo múltiple

$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_kx_k + u$$

Donde:

::: {.nonincremental}
* $y$ es la variable de respuesta que se desea predecir.
* $\beta_0, \beta_1, \beta_2,...,\beta_k$ son constantes desconocidas.
* $x_1, x_2, …, x_k$ son variables predictoras independientes que se miden sin error.
* u es el error de variable, que permite que cada respuesta se desvíe del valor promedio de $y$ en una cantidad $u$. Se debe suponer que los valores de u:

    1) son independientes;
    2) tienen una media de 0 y una varianza común $s^2$ para cualquier conjunto $x_1, x_2, …, x_k$, y
    3) están normalmente distribuidas.
:::

## Ejemplo modelo 2


```{r}
#| echo: true
load("datos/pisa18_2.RData")

y <- pisa18_2$PVREAD
x1 <- pisa18_2$PVMATH
x2 <- pisa18_2$ST013Q01TA

modelo2 <- lm(y ~ x1 + x2)
summary(modelo2)

```


## Ejemplo modelo 3


```{r}
#| echo: true

y <- pisa18_2$PVREAD
x1 <- pisa18_2$PVMATH
x2 <- pisa18_2$ST013Q01TA
x3 <- pisa18_2$MISCED

modelo3 <- lm(y ~ x1 + x2 + x3)
summary(modelo3)

```


## Ejemplo modelo 4


```{r}
#| echo: true

y <- pisa18_2$PVREAD
x1 <- pisa18_2$PVMATH
x2 <- pisa18_2$ST013Q01TA
x3 <- pisa18_2$MISCED
x4 <- pisa18_2$TMINS

modelo4 <- lm(y ~ x1 + x2 + x3 + x4)
summary(modelo4)

```



## Ejemplo modelo 5


```{r}
#| echo: true

y <- pisa18_2$PVREAD
x1 <- pisa18_2$PVMATH
x2 <- pisa18_2$ST013Q01TA
x3 <- pisa18_2$MISCED
x4 <- pisa18_2$TMINS

modelo5 <- lm(y ~ x2 + x3 + x4)
summary(modelo5)

```

